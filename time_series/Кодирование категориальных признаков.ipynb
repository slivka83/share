{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://inclass.kaggle.com/rohitgr/autoencoders-tsne\n",
    "- https://www.kaggle.com/waydeherman/tutorial-categorical-encoding\n",
    "- https://www.kaggle.com/waydeherman/categorical-feature-encoding\n",
    "- https://www.geeksforgeeks.org/ml-extra-tree-classifier-for-feature-selection/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во набора данных, с которыми Вам предстоит работать, будут содержаться категориальные переменные. Категориальные  переменными, это переменные, которые содержат текствые, а не числовые значения. Например:\n",
    "- [\"мужчина\", \"женщина\"]\n",
    "- [\"из Европы\", \"из США\", \"из Азии\"]\n",
    "\n",
    "Некоторые реализации алгоритмов машинного обучения могут работать с категориальными данными напрямую, производя преобразование к числовому виду самостоятельно. Другим же нужно предварительное преобразование категориальныех переменных к числовому виду."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import feather\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cd_code</th>\n",
       "      <td>AL-01</td>\n",
       "      <td>AL-02</td>\n",
       "      <td>AL-03</td>\n",
       "      <td>AL-04</td>\n",
       "      <td>AL-05</td>\n",
       "      <td>AL-06</td>\n",
       "      <td>AL-07</td>\n",
       "      <td>AK-AL</td>\n",
       "      <td>AZ-01</td>\n",
       "      <td>AZ-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster</th>\n",
       "      <td>Rural-suburban mix</td>\n",
       "      <td>Pure rural</td>\n",
       "      <td>Pure rural</td>\n",
       "      <td>Pure rural</td>\n",
       "      <td>Rural-suburban mix</td>\n",
       "      <td>Rural-suburban mix</td>\n",
       "      <td>Rural-suburban mix</td>\n",
       "      <td>Pure rural</td>\n",
       "      <td>Pure rural</td>\n",
       "      <td>Dense suburban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region</th>\n",
       "      <td>South</td>\n",
       "      <td>South</td>\n",
       "      <td>South</td>\n",
       "      <td>South</td>\n",
       "      <td>South</td>\n",
       "      <td>South</td>\n",
       "      <td>South</td>\n",
       "      <td>West</td>\n",
       "      <td>West</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dem_perc</th>\n",
       "      <td>36.8</td>\n",
       "      <td>38.4</td>\n",
       "      <td>36.2</td>\n",
       "      <td>20.1</td>\n",
       "      <td>38.9</td>\n",
       "      <td>30.8</td>\n",
       "      <td>97.8</td>\n",
       "      <td>46.5</td>\n",
       "      <td>53.8</td>\n",
       "      <td>54.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gop_perc</th>\n",
       "      <td>63.2</td>\n",
       "      <td>61.4</td>\n",
       "      <td>63.7</td>\n",
       "      <td>79.8</td>\n",
       "      <td>61</td>\n",
       "      <td>69.2</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1</td>\n",
       "      <td>46.1</td>\n",
       "      <td>45.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dem_margin</th>\n",
       "      <td>-26.4</td>\n",
       "      <td>-23</td>\n",
       "      <td>-27.5</td>\n",
       "      <td>-59.6</td>\n",
       "      <td>-22.1</td>\n",
       "      <td>-38.4</td>\n",
       "      <td>97.8</td>\n",
       "      <td>-6.6</td>\n",
       "      <td>7.7</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clinton_margin</th>\n",
       "      <td>-29.2</td>\n",
       "      <td>-31.7</td>\n",
       "      <td>-33</td>\n",
       "      <td>-62.5</td>\n",
       "      <td>-32.9</td>\n",
       "      <td>-43.8</td>\n",
       "      <td>41.2</td>\n",
       "      <td>-14.7</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swing</th>\n",
       "      <td>2.8</td>\n",
       "      <td>8.7</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.9</td>\n",
       "      <td>10.8</td>\n",
       "      <td>5.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.1</td>\n",
       "      <td>8.8</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 0           1           2           3  \\\n",
       "Unnamed: 0                       1           2           3           4   \n",
       "cd_code                      AL-01       AL-02       AL-03       AL-04   \n",
       "cluster         Rural-suburban mix  Pure rural  Pure rural  Pure rural   \n",
       "region                       South       South       South       South   \n",
       "dem_perc                      36.8        38.4        36.2        20.1   \n",
       "gop_perc                      63.2        61.4        63.7        79.8   \n",
       "dem_margin                   -26.4         -23       -27.5       -59.6   \n",
       "clinton_margin               -29.2       -31.7         -33       -62.5   \n",
       "swing                          2.8         8.7         5.5         2.9   \n",
       "\n",
       "                                 4                   5                   6  \\\n",
       "Unnamed: 0                       5                   6                   7   \n",
       "cd_code                      AL-05               AL-06               AL-07   \n",
       "cluster         Rural-suburban mix  Rural-suburban mix  Rural-suburban mix   \n",
       "region                       South               South               South   \n",
       "dem_perc                      38.9                30.8                97.8   \n",
       "gop_perc                        61                69.2                   0   \n",
       "dem_margin                   -22.1               -38.4                97.8   \n",
       "clinton_margin               -32.9               -43.8                41.2   \n",
       "swing                         10.8                 5.4                 NaN   \n",
       "\n",
       "                         7           8               9  \n",
       "Unnamed: 0               8           9              10  \n",
       "cd_code              AK-AL       AZ-01           AZ-02  \n",
       "cluster         Pure rural  Pure rural  Dense suburban  \n",
       "region                West        West            West  \n",
       "dem_perc              46.5        53.8            54.7  \n",
       "gop_perc              53.1        46.1            45.2  \n",
       "dem_margin            -6.6         7.7             9.5  \n",
       "clinton_margin       -14.7        -1.1             4.8  \n",
       "swing                  8.1         8.8             4.7  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/district_density.csv')\n",
    "df.head(10).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Rural-suburban mix', 'Pure rural', 'Dense suburban',\n",
       "       'Urban-suburban mix', 'Pure urban', 'Sparse suburban'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cluster'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>total_bill</th>\n",
       "      <td>16.99</td>\n",
       "      <td>10.34</td>\n",
       "      <td>21.01</td>\n",
       "      <td>23.68</td>\n",
       "      <td>24.59</td>\n",
       "      <td>25.29</td>\n",
       "      <td>8.77</td>\n",
       "      <td>26.88</td>\n",
       "      <td>15.04</td>\n",
       "      <td>14.78</td>\n",
       "      <td>10.27</td>\n",
       "      <td>35.26</td>\n",
       "      <td>15.42</td>\n",
       "      <td>18.43</td>\n",
       "      <td>14.83</td>\n",
       "      <td>21.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tip</th>\n",
       "      <td>1.01</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.31</td>\n",
       "      <td>3.61</td>\n",
       "      <td>4.71</td>\n",
       "      <td>2</td>\n",
       "      <td>3.12</td>\n",
       "      <td>1.96</td>\n",
       "      <td>3.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>5</td>\n",
       "      <td>1.57</td>\n",
       "      <td>3</td>\n",
       "      <td>3.02</td>\n",
       "      <td>3.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoker</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>Sun</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Sun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>Dinner</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>Dinner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0       1       2       3       4       5       6       7  \\\n",
       "total_bill   16.99   10.34   21.01   23.68   24.59   25.29    8.77   26.88   \n",
       "tip           1.01    1.66     3.5    3.31    3.61    4.71       2    3.12   \n",
       "sex         Female    Male    Male    Male  Female    Male    Male    Male   \n",
       "smoker          No      No      No      No      No      No      No      No   \n",
       "day            Sun     Sun     Sun     Sun     Sun     Sun     Sun     Sun   \n",
       "time        Dinner  Dinner  Dinner  Dinner  Dinner  Dinner  Dinner  Dinner   \n",
       "size             2       3       3       2       4       4       2       4   \n",
       "\n",
       "                 8       9      10      11      12      13      14      15  \n",
       "total_bill   15.04   14.78   10.27   35.26   15.42   18.43   14.83   21.58  \n",
       "tip           1.96    3.23    1.71       5    1.57       3    3.02    3.92  \n",
       "sex           Male    Male    Male  Female    Male    Male  Female    Male  \n",
       "smoker          No      No      No      No      No      No      No      No  \n",
       "day            Sun     Sun     Sun     Sun     Sun     Sun     Sun     Sun  \n",
       "time        Dinner  Dinner  Dinner  Dinner  Dinner  Dinner  Dinner  Dinner  \n",
       "size             2       2       2       4       2       4       2       2  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/tips.csv')\n",
    "df.head(16).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Бинарное"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Loan_ID</th>\n",
       "      <td>LP001002</td>\n",
       "      <td>LP001003</td>\n",
       "      <td>LP001005</td>\n",
       "      <td>LP001006</td>\n",
       "      <td>LP001008</td>\n",
       "      <td>LP001011</td>\n",
       "      <td>LP001013</td>\n",
       "      <td>LP001014</td>\n",
       "      <td>LP001018</td>\n",
       "      <td>LP001020</td>\n",
       "      <td>LP001024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Married</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dependents</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3+</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Education</th>\n",
       "      <td>Graduate</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Not Graduate</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Not Graduate</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Graduate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Self_Employed</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <td>5849</td>\n",
       "      <td>4583</td>\n",
       "      <td>3000</td>\n",
       "      <td>2583</td>\n",
       "      <td>6000</td>\n",
       "      <td>5417</td>\n",
       "      <td>2333</td>\n",
       "      <td>3036</td>\n",
       "      <td>4006</td>\n",
       "      <td>12841</td>\n",
       "      <td>3200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <td>0</td>\n",
       "      <td>1508</td>\n",
       "      <td>0</td>\n",
       "      <td>2358</td>\n",
       "      <td>0</td>\n",
       "      <td>4196</td>\n",
       "      <td>1516</td>\n",
       "      <td>2504</td>\n",
       "      <td>1526</td>\n",
       "      <td>10968</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LoanAmount</th>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>66</td>\n",
       "      <td>120</td>\n",
       "      <td>141</td>\n",
       "      <td>267</td>\n",
       "      <td>95</td>\n",
       "      <td>158</td>\n",
       "      <td>168</td>\n",
       "      <td>349</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Credit_History</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Property_Area</th>\n",
       "      <td>Urban</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Semiurban</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Semiurban</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loan_Status</th>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0         1         2             3         4  \\\n",
       "Loan_ID            LP001002  LP001003  LP001005      LP001006  LP001008   \n",
       "Gender                 Male      Male      Male          Male      Male   \n",
       "Married                  No       Yes       Yes           Yes        No   \n",
       "Dependents                0         1         0             0         0   \n",
       "Education          Graduate  Graduate  Graduate  Not Graduate  Graduate   \n",
       "Self_Employed            No        No       Yes            No        No   \n",
       "ApplicantIncome        5849      4583      3000          2583      6000   \n",
       "CoapplicantIncome         0      1508         0          2358         0   \n",
       "LoanAmount              NaN       128        66           120       141   \n",
       "Loan_Amount_Term        360       360       360           360       360   \n",
       "Credit_History            1         1         1             1         1   \n",
       "Property_Area         Urban     Rural     Urban         Urban     Urban   \n",
       "Loan_Status               Y         N         Y             Y         Y   \n",
       "\n",
       "                          5             6          7         8          9  \\\n",
       "Loan_ID            LP001011      LP001013   LP001014  LP001018   LP001020   \n",
       "Gender                 Male          Male       Male      Male       Male   \n",
       "Married                 Yes           Yes        Yes       Yes        Yes   \n",
       "Dependents                2             0         3+         2          1   \n",
       "Education          Graduate  Not Graduate   Graduate  Graduate   Graduate   \n",
       "Self_Employed           Yes            No         No        No         No   \n",
       "ApplicantIncome        5417          2333       3036      4006      12841   \n",
       "CoapplicantIncome      4196          1516       2504      1526      10968   \n",
       "LoanAmount              267            95        158       168        349   \n",
       "Loan_Amount_Term        360           360        360       360        360   \n",
       "Credit_History            1             1          0         1          1   \n",
       "Property_Area         Urban         Urban  Semiurban     Urban  Semiurban   \n",
       "Loan_Status               Y             Y          N         Y          N   \n",
       "\n",
       "                         10  \n",
       "Loan_ID            LP001024  \n",
       "Gender                 Male  \n",
       "Married                 Yes  \n",
       "Dependents                2  \n",
       "Education          Graduate  \n",
       "Self_Employed            No  \n",
       "ApplicantIncome        3200  \n",
       "CoapplicantIncome       700  \n",
       "LoanAmount               70  \n",
       "Loan_Amount_Term        360  \n",
       "Credit_History            1  \n",
       "Property_Area         Urban  \n",
       "Loan_Status               Y  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/shri1407/Loan-Prediction-Dataset/master/train.csv')\n",
    "df.head(11).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Loan_ID</th>\n",
       "      <td>LP001002</td>\n",
       "      <td>LP001003</td>\n",
       "      <td>LP001005</td>\n",
       "      <td>LP001006</td>\n",
       "      <td>LP001008</td>\n",
       "      <td>LP001011</td>\n",
       "      <td>LP001013</td>\n",
       "      <td>LP001014</td>\n",
       "      <td>LP001018</td>\n",
       "      <td>LP001020</td>\n",
       "      <td>LP001024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Married</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dependents</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3+</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Education</th>\n",
       "      <td>Graduate</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Not Graduate</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Not Graduate</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Graduate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Self_Employed</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <td>5849</td>\n",
       "      <td>4583</td>\n",
       "      <td>3000</td>\n",
       "      <td>2583</td>\n",
       "      <td>6000</td>\n",
       "      <td>5417</td>\n",
       "      <td>2333</td>\n",
       "      <td>3036</td>\n",
       "      <td>4006</td>\n",
       "      <td>12841</td>\n",
       "      <td>3200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <td>0</td>\n",
       "      <td>1508</td>\n",
       "      <td>0</td>\n",
       "      <td>2358</td>\n",
       "      <td>0</td>\n",
       "      <td>4196</td>\n",
       "      <td>1516</td>\n",
       "      <td>2504</td>\n",
       "      <td>1526</td>\n",
       "      <td>10968</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LoanAmount</th>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>66</td>\n",
       "      <td>120</td>\n",
       "      <td>141</td>\n",
       "      <td>267</td>\n",
       "      <td>95</td>\n",
       "      <td>158</td>\n",
       "      <td>168</td>\n",
       "      <td>349</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Credit_History</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Property_Area</th>\n",
       "      <td>Urban</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Semiurban</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Semiurban</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loan_Status</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0         1         2             3         4  \\\n",
       "Loan_ID            LP001002  LP001003  LP001005      LP001006  LP001008   \n",
       "Gender                    0         0         0             0         0   \n",
       "Married                   0         1         1             1         0   \n",
       "Dependents                0         1         0             0         0   \n",
       "Education          Graduate  Graduate  Graduate  Not Graduate  Graduate   \n",
       "Self_Employed            No        No       Yes            No        No   \n",
       "ApplicantIncome        5849      4583      3000          2583      6000   \n",
       "CoapplicantIncome         0      1508         0          2358         0   \n",
       "LoanAmount              NaN       128        66           120       141   \n",
       "Loan_Amount_Term        360       360       360           360       360   \n",
       "Credit_History            1         1         1             1         1   \n",
       "Property_Area         Urban     Rural     Urban         Urban     Urban   \n",
       "Loan_Status               1         0         1             1         1   \n",
       "\n",
       "                          5             6          7         8          9  \\\n",
       "Loan_ID            LP001011      LP001013   LP001014  LP001018   LP001020   \n",
       "Gender                    0             0          0         0          0   \n",
       "Married                   1             1          1         1          1   \n",
       "Dependents                2             0         3+         2          1   \n",
       "Education          Graduate  Not Graduate   Graduate  Graduate   Graduate   \n",
       "Self_Employed           Yes            No         No        No         No   \n",
       "ApplicantIncome        5417          2333       3036      4006      12841   \n",
       "CoapplicantIncome      4196          1516       2504      1526      10968   \n",
       "LoanAmount              267            95        158       168        349   \n",
       "Loan_Amount_Term        360           360        360       360        360   \n",
       "Credit_History            1             1          0         1          1   \n",
       "Property_Area         Urban         Urban  Semiurban     Urban  Semiurban   \n",
       "Loan_Status               1             1          0         1          0   \n",
       "\n",
       "                         10  \n",
       "Loan_ID            LP001024  \n",
       "Gender                    0  \n",
       "Married                   1  \n",
       "Dependents                2  \n",
       "Education          Graduate  \n",
       "Self_Employed            No  \n",
       "ApplicantIncome        3200  \n",
       "CoapplicantIncome       700  \n",
       "LoanAmount               70  \n",
       "Loan_Amount_Term        360  \n",
       "Credit_History            1  \n",
       "Property_Area         Urban  \n",
       "Loan_Status               1  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Gender'] = df['Gender'].map({'Male':0,'Female':1})\n",
    "df['Married'] = df['Married'].map({'No':0,'Yes':1})\n",
    "df['Loan_Status'] = df['Loan_Status'].map({'N':0,'Y':1})\n",
    "\n",
    "df.head(11).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LE (label encoding) is the most simple. We have some categories (country for example) ['Russia', 'USA', 'GB']. But algoritms do not work with strings, they need numbers. Ok, we can do it ['Russia', 'USA', 'GB'] -> [0, 1, 2]. Relly simple. Let's try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cd_code</th>\n",
       "      <td>AL-01</td>\n",
       "      <td>AL-02</td>\n",
       "      <td>AL-03</td>\n",
       "      <td>AL-04</td>\n",
       "      <td>AL-05</td>\n",
       "      <td>AL-06</td>\n",
       "      <td>AL-07</td>\n",
       "      <td>AK-AL</td>\n",
       "      <td>AZ-01</td>\n",
       "      <td>AZ-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster</th>\n",
       "      <td>Rural-suburban mix</td>\n",
       "      <td>Pure rural</td>\n",
       "      <td>Pure rural</td>\n",
       "      <td>Pure rural</td>\n",
       "      <td>Rural-suburban mix</td>\n",
       "      <td>Rural-suburban mix</td>\n",
       "      <td>Rural-suburban mix</td>\n",
       "      <td>Pure rural</td>\n",
       "      <td>Pure rural</td>\n",
       "      <td>Dense suburban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region</th>\n",
       "      <td>South</td>\n",
       "      <td>South</td>\n",
       "      <td>South</td>\n",
       "      <td>South</td>\n",
       "      <td>South</td>\n",
       "      <td>South</td>\n",
       "      <td>South</td>\n",
       "      <td>West</td>\n",
       "      <td>West</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dem_perc</th>\n",
       "      <td>36.8</td>\n",
       "      <td>38.4</td>\n",
       "      <td>36.2</td>\n",
       "      <td>20.1</td>\n",
       "      <td>38.9</td>\n",
       "      <td>30.8</td>\n",
       "      <td>97.8</td>\n",
       "      <td>46.5</td>\n",
       "      <td>53.8</td>\n",
       "      <td>54.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gop_perc</th>\n",
       "      <td>63.2</td>\n",
       "      <td>61.4</td>\n",
       "      <td>63.7</td>\n",
       "      <td>79.8</td>\n",
       "      <td>61</td>\n",
       "      <td>69.2</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1</td>\n",
       "      <td>46.1</td>\n",
       "      <td>45.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dem_margin</th>\n",
       "      <td>-26.4</td>\n",
       "      <td>-23</td>\n",
       "      <td>-27.5</td>\n",
       "      <td>-59.6</td>\n",
       "      <td>-22.1</td>\n",
       "      <td>-38.4</td>\n",
       "      <td>97.8</td>\n",
       "      <td>-6.6</td>\n",
       "      <td>7.7</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clinton_margin</th>\n",
       "      <td>-29.2</td>\n",
       "      <td>-31.7</td>\n",
       "      <td>-33</td>\n",
       "      <td>-62.5</td>\n",
       "      <td>-32.9</td>\n",
       "      <td>-43.8</td>\n",
       "      <td>41.2</td>\n",
       "      <td>-14.7</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swing</th>\n",
       "      <td>2.8</td>\n",
       "      <td>8.7</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.9</td>\n",
       "      <td>10.8</td>\n",
       "      <td>5.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.1</td>\n",
       "      <td>8.8</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                1           2           3           4   \\\n",
       "cd_code                      AL-01       AL-02       AL-03       AL-04   \n",
       "cluster         Rural-suburban mix  Pure rural  Pure rural  Pure rural   \n",
       "region                       South       South       South       South   \n",
       "dem_perc                      36.8        38.4        36.2        20.1   \n",
       "gop_perc                      63.2        61.4        63.7        79.8   \n",
       "dem_margin                   -26.4         -23       -27.5       -59.6   \n",
       "clinton_margin               -29.2       -31.7         -33       -62.5   \n",
       "swing                          2.8         8.7         5.5         2.9   \n",
       "\n",
       "                                5                   6                   7   \\\n",
       "cd_code                      AL-05               AL-06               AL-07   \n",
       "cluster         Rural-suburban mix  Rural-suburban mix  Rural-suburban mix   \n",
       "region                       South               South               South   \n",
       "dem_perc                      38.9                30.8                97.8   \n",
       "gop_perc                        61                69.2                   0   \n",
       "dem_margin                   -22.1               -38.4                97.8   \n",
       "clinton_margin               -32.9               -43.8                41.2   \n",
       "swing                         10.8                 5.4                 NaN   \n",
       "\n",
       "                        8           9               10  \n",
       "cd_code              AK-AL       AZ-01           AZ-02  \n",
       "cluster         Pure rural  Pure rural  Dense suburban  \n",
       "region                West        West            West  \n",
       "dem_perc              46.5        53.8            54.7  \n",
       "gop_perc              53.1        46.1            45.2  \n",
       "dem_margin            -6.6         7.7             9.5  \n",
       "clinton_margin       -14.7        -1.1             4.8  \n",
       "swing                  8.1         8.8             4.7  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/district_density.csv', index_col=0)\n",
    "df.head(10).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ручное"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autor_to_int = dict((zip(train_df.author.unique(), range(train_df.author.unique().shape[0]))))\n",
    "domain_to_int = dict((zip(train_df.domain.unique(), range(train_df.domain.unique().shape[0]))))\n",
    "lang_to_int = dict((zip(train_df.lang.unique(), range(train_df.lang.unique().shape[0]))))\n",
    "train_df_le = train_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Библиотека"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Alcohol and Drug Use', 'Blood Diseases', 'Circulatory System'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Classification'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n",
    "\n",
    "list(le.classes_)\n",
    "['amsterdam', 'paris', 'tokyo']\n",
    "le.transform([\"tokyo\", \"tokyo\", \"paris\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokyo', 'tokyo', 'paris']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(le.inverse_transform([2, 2, 1]))\n",
    "['tokyo', 'tokyo', 'paris']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Даже категориальные фичи представленные целыми числами имеют те же недостатки. 2 > 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequncy Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can treat category as the thing on its own. ['Russia', 'USA', 'GB'] will convert to 3 features, each of which would take value 0 or 1.\n",
    "\n",
    "This way we can treat features independently, but cardinality blows up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n",
    "enc.fit(X)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.transform([['female', 'from US', 'uses Safari'],['male', 'from Europe', 'uses Safari']]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_ohe = train_df.copy()\n",
    "y = train_df_ohe.log_recommends\n",
    "X = train_df_ohe.drop('log_recommends', axis=1)\n",
    "X[X.columns] = X[X.columns].astype('category')\n",
    "X = pd.get_dummies(X, prefix=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Корелирумые фичи плохи для линейных моделей..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/mahendrabishnoi2/06-intermediate-ml/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет медиум"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You already knew everything that was above.\n",
    "\n",
    "Now it's time to try something new. We'll look at NN approach to categorical variables.\n",
    "\n",
    "In kaggle competitions, we can see, that in competitions with heavy use of categorical data tree ensembling methods work the best (XGBoost). Why in ages of rising NN they still haven't conquered this area?<br>\n",
    "In principle a neural network can approximate any continuous function and piecewise continuous function. However, it is not suitable to approximate arbitrary non-continuous functions as it assumes a certain level of continuity in its general form. During the training phase the continuity of the data guarantees the convergence of the optimization, and during the prediction phase it ensures that slightly changing the values of the input keeps the output stable.<br>\n",
    "Trees don't have this assumption about data continuity and can divide the states of a variable as fine as necessary.\n",
    "\n",
    "NN is somehow close to the linear model. What have we done to linear model? We used OHE, but it blew our dimensionality. For many real-world tasks when features may have cardinality about millions it would be harder. Secondly, we've lost some information with such a transformation. In our example, we have language as a feature. When we are converting \"SPANISH\" -> [1,0,0,...,0] and when \"ENGLISH\" -> [0,1,0,...,0]. Both languages have the same distance between each other, but there is no doubts Spanish and English are more similar than English and Chinese. We want to get this inner relation.\n",
    "\n",
    "The solution to these problems is to use <b>embeddings</b>, which translate large sparse vectors into a lower-dimensional space that preserves semantic relationships.\n",
    "\n",
    "How it works in NLP field:\n",
    "\n",
    "| feature | vector   |\n",
    "|------|------|\n",
    "| puppy | [0.9, 1.0, 0.0]   |\n",
    "|   dog  | [1.0, 0.2, 0.0]|\n",
    "|   kitten  | [0.0, 1.0, 0.9]|\n",
    "|   cat  | [0.0, 0.2, 1.0]|\n",
    "\n",
    "We see words share some values, that we can consider as \"dogness\" or \"size\".\n",
    "\n",
    "To do this, all we need is the matrix of embeddings.\n",
    "\n",
    "At the start, we are applying OHE and obtaining N rows with M columns. Where m is a category value. Then we picking row that encodes our category from the embedding matrix. Further we using this vector that repsents some rich properties of our initial category.  \n",
    "We can obtain embeddings with NN magic. We are training embedding matrix with the size of MxP where P is number which we are picking (hyperparameter). Google's heuristic says us to pick M**0.25\n",
    "\n",
    "<img src='https://habrastorage.org/webt/of/jy/gd/ofjygd5fmbpxwz8x6boeu2nnpk4.png' />\n",
    "\n",
    "I'll use keras, but it's not important it's just a tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.layers import Input, Embedding, Dense, Dropout\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class EmbeddingMapping():\n",
    "    \"\"\"\n",
    "    Helper class for handling categorical variables\n",
    "    An instance of this class should be defined for each categorical variable we want to use.\n",
    "    \"\"\"\n",
    "    def __init__(self, series):\n",
    "        # get a list of unique values\n",
    "        values = series.unique().tolist()\n",
    "        \n",
    "        # Set a dictionary mapping from values to integer value\n",
    "        self.embedding_dict = {value: int_value+1 for int_value, value in enumerate(values)}\n",
    "        \n",
    "        # The num_values will be used as the input_dim when defining the embedding layer. \n",
    "        # It will also be returned for unseen values \n",
    "        self.num_values = len(values) + 1\n",
    "\n",
    "    def get_mapping(self, value):\n",
    "        # If the value was seen in the training set, return its integer mapping\n",
    "        if value in self.embedding_dict:\n",
    "            return self.embedding_dict[value]\n",
    "        # Else, return the same integer for unseen values\n",
    "        else:\n",
    "            return self.num_values\n",
    "\n",
    "\n",
    "#converting some out features\n",
    "author_mapping = EmbeddingMapping(train_df['author'])\n",
    "domain_mapping = EmbeddingMapping(train_df['domain'])\n",
    "lang_mapping = EmbeddingMapping(train_df['lang'])\n",
    "X_emb = X_emb.assign(author_mapping=X_emb['author'].apply(author_mapping.get_mapping))\n",
    "X_emb = X_emb.assign(lang_mapping=X_emb['lang'].apply(lang_mapping.get_mapping))\n",
    "X_emb = X_emb.assign(domain_mapping=X_emb['domain'].apply(domain_mapping.get_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_emb.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_emb = train_df.copy()\n",
    "\n",
    "X_train, X_val,y_train,y_val = train_test_split(X_emb,y, test_size=0.2)\n",
    "\n",
    "# Keras functional API\n",
    "#Input\n",
    "author_input = Input(shape=(1,), dtype='int32') \n",
    "lang_input = Input(shape=(1,), dtype='int32')\n",
    "domain_input = Input(shape=(1,), dtype='int32')\n",
    "\n",
    "# It's google's fule of thumb N_embeddings == N_originall_dim**0.25\n",
    "# Let’s define the embedding layer and flatten it\n",
    "# Originally 31331 unique authors\n",
    "author_embedings = Embedding(output_dim=13, input_dim=author_mapping.num_values, input_length=1)(author_input)\n",
    "author_embedings = keras.layers.Reshape((13,))(author_embedings)\n",
    "# Originally 62 unique langs\n",
    "lang_embedings = Embedding(output_dim=3, input_dim=lang_mapping.num_values, input_length=1)(lang_input)\n",
    "lang_embedings = keras.layers.Reshape((3,))(lang_embedings)\n",
    "# Originally 221 unique domains\n",
    "domain_embedings = Embedding(output_dim=4, input_dim=domain_mapping.num_values, input_length=1)(domain_input)\n",
    "domain_embedings = keras.layers.Reshape((4,))(domain_embedings)\n",
    "\n",
    "\n",
    "# Concatenate continuous and embeddings inputs\n",
    "all_input = keras.layers.concatenate([lang_embedings, author_embedings, domain_embedings])\n",
    "\n",
    "# Fully connected layer to train NN and learn embeddings\n",
    "units=25\n",
    "dense1 = Dense(units=units, activation='relu')(all_input)\n",
    "dense1 = Dropout(0.5)(dense1)\n",
    "dense2 = Dense(units, activation='relu')(dense1)\n",
    "dense2 = Dropout(0.5)(dense2)\n",
    "predictions = Dense(1)(dense2)\n",
    "\n",
    "epochs = 40\n",
    "model = Model(inputs=[lang_input, author_input, domain_input], outputs=predictions)\n",
    "model.compile(loss='mae', optimizer='adagrad')\n",
    "\n",
    "history = model.fit([X_train['lang_mapping'], X_train['author_mapping'], X_train['domain_mapping']], y_train, \n",
    "          epochs=epochs, batch_size=128, verbose=0,\n",
    "          validation_data=([X_val['lang_mapping'], X_val['author_mapping'], X_val['domain_mapping']], y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this step, we've trained a NN, but we are not going to use it. We want to get the embeddings layer.\n",
    "\n",
    "For each category, we have distinct embedding. Let's extract them and use it in our simple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[5].get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_embedding = model.layers[3].get_weights()[0]\n",
    "lang_emb_cols = [f'lang_emb_{i}' for i in range(lang_embedding.shape[1])]\n",
    "\n",
    "author_embedding = model.layers[4].get_weights()[0]\n",
    "aut_emb_cols = [f'aut_emb_{i}' for i in range(author_embedding.shape[1])]\n",
    "\n",
    "domain_embedding = model.layers[5].get_weights()[0]\n",
    "dom_emb_cols = [f'dom_emb_{i}' for i in range(domain_embedding.shape[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have embeddings, and all we need is to take a row that corresponds to our examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author_vector(aut_num):\n",
    "    return author_embedding[aut_num,:]\n",
    "def get_lang_vector(lang_num):\n",
    "    return lang_embedding[lang_num,:]\n",
    "def get_domain_vector(dom_num):\n",
    "    return domain_embedding[dom_num,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_lang_vector(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_emb = pd.DataFrame(X_emb['lang_mapping'].apply(get_lang_vector).values.tolist(), columns=lang_emb_cols)\n",
    "lang_emb.index = X_emb.index\n",
    "X_emb[lang_emb_cols] = lang_emb\n",
    "\n",
    "aut_emb = pd.DataFrame(X_emb['author_mapping'].apply(get_author_vector).values.tolist(), columns=aut_emb_cols)\n",
    "aut_emb.index = X_emb.index\n",
    "X_emb[aut_emb_cols] = aut_emb\n",
    "\n",
    "dom_emb = pd.DataFrame(X_emb['domain_mapping'].apply(get_domain_vector).values.tolist(), columns=dom_emb_cols)\n",
    "dom_emb.index = X_emb.index\n",
    "X_emb[dom_emb_cols] = dom_emb\n",
    "\n",
    "X_emb.drop(['author', 'lang', 'domain', 'log_recommends',\n",
    "           'author_mapping', 'lang_mapping', 'domain_mapping',],axis=1, inplace=True)\n",
    "\n",
    "X_emb.columns\n",
    "\n",
    "X_train, X_val,y_train,y_val = train_test_split(X_emb,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "preds = rf.predict(X_val)\n",
    "mean_absolute_error(y_val, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge()\n",
    "ridge.fit(X_train, y_train)\n",
    "preds = ridge.predict(X_val)\n",
    "mean_absolute_error(y_val, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like a success.\n",
    "\n",
    "One nice property of embeddings - our categories have some simularity(distance) from each other. Let's look at the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.models as bm, bokeh.plotting as pl\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
    "                 width=600, height=400, show=True, **kwargs):\n",
    "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
    "    if isinstance(color, str): color = [color] * len(x)\n",
    "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
    "\n",
    "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
    "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
    "\n",
    "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
    "    if show: pl.show(fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs_vectors = [get_lang_vector(l) for l in lang_mapping.embedding_dict.values()]\n",
    "\n",
    "lang_tsne = TSNE().fit_transform(langs_vectors )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_vectors(lang_tsne[:, 0], lang_tsne[:, 1], token=list(lang_mapping.embedding_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs_vectors_pca = PCA(n_components=2).fit_transform(langs_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_vectors(langs_vectors_pca[:, 0], langs_vectors_pca[:, 1], token=list(lang_mapping.embedding_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time graphs doesn't look any meaningfull, but score speaks for itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature transformations with ensembles of trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform your features into a higher dimensional, sparse space. Then train a linear model on these features.\n",
    "\n",
    "First fit an ensemble of trees (totally random trees, a random forest, or gradient boosted trees) on the training set. Then each leaf of each tree in the ensemble is assigned a fixed arbitrary feature index in a new feature space. These leaf indices are then encoded in a one-hot fashion.\n",
    "\n",
    "Each sample goes through the decisions of each tree of the ensemble and ends up in one leaf per tree. The sample is encoded by setting feature values for these leaves to 1 and the other feature values to 0.\n",
    "\n",
    "The resulting transformer has then learned a supervised, sparse, high-dimensional categorical embedding of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,\n",
    "                              GradientBoostingClassifier)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "n_estimator = 10\n",
    "X, y = make_classification(n_samples=80000)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "# It is important to train the ensemble of trees on a different subset\n",
    "# of the training data than the linear regression model to avoid\n",
    "# overfitting, in particular if the total number of leaves is\n",
    "# similar to the number of training samples\n",
    "X_train, X_train_lr, y_train, y_train_lr = train_test_split(\n",
    "    X_train, y_train, test_size=0.5)\n",
    "\n",
    "# Unsupervised transformation based on totally random trees\n",
    "rt = RandomTreesEmbedding(max_depth=3, n_estimators=n_estimator,\n",
    "                          random_state=0)\n",
    "\n",
    "rt_lm = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "pipeline = make_pipeline(rt, rt_lm)\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred_rt = pipeline.predict_proba(X_test)[:, 1]\n",
    "fpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_test, y_pred_rt)\n",
    "\n",
    "# Supervised transformation based on random forests\n",
    "rf = RandomForestClassifier(max_depth=3, n_estimators=n_estimator)\n",
    "rf_enc = OneHotEncoder(categories='auto')\n",
    "rf_lm = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_enc.fit(rf.apply(X_train))\n",
    "rf_lm.fit(rf_enc.transform(rf.apply(X_train_lr)), y_train_lr)\n",
    "\n",
    "y_pred_rf_lm = rf_lm.predict_proba(rf_enc.transform(rf.apply(X_test)))[:, 1]\n",
    "fpr_rf_lm, tpr_rf_lm, _ = roc_curve(y_test, y_pred_rf_lm)\n",
    "\n",
    "# Supervised transformation based on gradient boosted trees\n",
    "grd = GradientBoostingClassifier(n_estimators=n_estimator)\n",
    "grd_enc = OneHotEncoder(categories='auto')\n",
    "grd_lm = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "grd.fit(X_train, y_train)\n",
    "grd_enc.fit(grd.apply(X_train)[:, :, 0])\n",
    "grd_lm.fit(grd_enc.transform(grd.apply(X_train_lr)[:, :, 0]), y_train_lr)\n",
    "\n",
    "y_pred_grd_lm = grd_lm.predict_proba(\n",
    "    grd_enc.transform(grd.apply(X_test)[:, :, 0]))[:, 1]\n",
    "fpr_grd_lm, tpr_grd_lm, _ = roc_curve(y_test, y_pred_grd_lm)\n",
    "\n",
    "# The gradient boosted model by itself\n",
    "y_pred_grd = grd.predict_proba(X_test)[:, 1]\n",
    "fpr_grd, tpr_grd, _ = roc_curve(y_test, y_pred_grd)\n",
    "\n",
    "# The random forest model by itself\n",
    "y_pred_rf = rf.predict_proba(X_test)[:, 1]\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_rf)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_rt_lm, tpr_rt_lm, label='RT + LR')\n",
    "plt.plot(fpr_rf, tpr_rf, label='RF')\n",
    "plt.plot(fpr_rf_lm, tpr_rf_lm, label='RF + LR')\n",
    "plt.plot(fpr_grd, tpr_grd, label='GBT')\n",
    "plt.plot(fpr_grd_lm, tpr_grd_lm, label='GBT + LR')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(2)\n",
    "plt.xlim(0, 0.2)\n",
    "plt.ylim(0.8, 1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_rt_lm, tpr_rt_lm, label='RT + LR')\n",
    "plt.plot(fpr_rf, tpr_rf, label='RF')\n",
    "plt.plot(fpr_rf_lm, tpr_rf_lm, label='RF + LR')\n",
    "plt.plot(fpr_grd, tpr_grd, label='GBT')\n",
    "plt.plot(fpr_grd_lm, tpr_grd_lm, label='GBT + LR')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve (zoomed in at top left)')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cat2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Медиум"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach came from NLP is word2Vec that was renamed to Cat2Vec. It haven't firm confirmation about it's usefulness, but there are some papers that argue that. (Links below).\n",
    "\n",
    "Distributional semantics and John Rupert Firth says \"You shall know a word by the company it keeps\". Some words share the same context, so they are somehow similar. We can suggest, that categories may share some inner correlation by they co-occurrence. For example weather and city. Maybe city \"Philadelphia\" may be similar to weather \"always sunny\", or \"Moskow\" with \"snowy\".\n",
    "\n",
    "Firstly we applying Feature encoding, then we can make \"sentence\" from our row.\n",
    "\n",
    "In the example below, let's imagine we have an article at \"Monday January 2018 English_language Medium.com\" Here our sentence so maybe if English co-occurs with Medium more often then Chinese with hackernoon.com. (Poor consideration but just for example).\n",
    "\n",
    "The only consideration is \"word\" order. Word2Vec relays on order, fro categorical \"sentence\" it doesn't matter, so it's better to shuffle sentences.\n",
    "\n",
    "Let's implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v = train_df.copy()\n",
    "\n",
    "month_int_to_name = {1:'jan',2:'feb',3:'apr',4:'march',5:'may',6:'june',7:'jul',8:'aug',9:'sept',10:'okt',11:'nov',12:'dec',}\n",
    "weekday_int_to_day = {0:'mon',1:'thus',2:'wen',3:'thusd',4:'fri',5:'sut',6:'sun',}\n",
    "\n",
    "working_day_int_to_day = {1: 'work',0:'not_work'}\n",
    "\n",
    "working_day_int_to_day = {1: 'work',0:'not_work'}\n",
    "X_w2v.weekday = X_w2v.weekday.apply(lambda x : weekday_int_to_day[x])\n",
    "X_w2v.working_day = X_w2v.working_day.apply(lambda x : working_day_int_to_day[x])\n",
    "\n",
    "all_list = list()\n",
    "for ind, r in X_w2v.iterrows():\n",
    "    values_list = [str(val).replace(' ', '_') for val in r.values]\n",
    "    all_list.append(values_list)\n",
    "    \n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(all_list, \n",
    "                 size=32,      # embedding vector size\n",
    "                 min_count=5,  # consider words that occured at least 5 times\n",
    "                 window=5).wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('june')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sorted(model.vocab.keys(), \n",
    "               key=lambda word: model.vocab[word].count,\n",
    "               reverse=True)[:1000]\n",
    "\n",
    "print(words[::100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = np.array([model.get_vector(wrd) for wrd in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw a graph as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tsne = TSNE().fit_transform(word_vectors )\n",
    "\n",
    "draw_vectors(word_tsne[:, 0], word_tsne[:, 1], color='green', token=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our categories mingled, but we can notice that years, days, languages are stays apart from authors cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrase_embedding(phrase):\n",
    "    \"\"\"\n",
    "    Convert phrase to a vector by aggregating it's word embeddings. See description above.\n",
    "    \"\"\"\n",
    "    # 1. lowercase phrase\n",
    "    # 2. tokenize phrase\n",
    "    # 3. average word vectors for all words in tokenized phrase\n",
    "    # skip words that are not in model's vocabulary\n",
    "    # if all words are missing from vocabulary, return zeros\n",
    "    \n",
    "    vector = np.zeros([model.vector_size], dtype='float32')\n",
    "    word_count = 0\n",
    "    \n",
    "    for word in phrase.split():\n",
    "        if word in model.vocab:\n",
    "            vector += model.get_vector(word)\n",
    "            word_count += 1\n",
    "    \n",
    "    if word_count:\n",
    "        vector /= word_count\n",
    "        \n",
    "            \n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = list()\n",
    "for ph in all_list:\n",
    "    vector = get_phrase_embedding(' '.join(ph))\n",
    "    new_features.append(vector)\n",
    "    \n",
    "\n",
    "new_features = pd.DataFrame(new_features)\n",
    "new_features.index = X_w2v.index\n",
    "X_w2v = pd.concat([X_w2v,new_features],axis=1\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v.drop(['author','domain','lang','working_day','year','month','weekday','log_recommends'], axis=1, inplace = True)\n",
    "\n",
    "X_train, X_val,y_train,y_val = train_test_split(X_w2v,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "preds = rf.predict(X_val)\n",
    "mean_absolute_error(y_val, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge()\n",
    "ridge.fit(X_train, y_train)\n",
    "preds = ridge.predict(X_val)\n",
    "mean_absolute_error(y_val, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poor result, but I cutted a lot of features that could help this algorithm to word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/ftp/arxiv/papers/1603/1603.04259.pdf ITEM2VEC: NEURAL ITEM EMBEDDING FOR COLLABORATIVE FILTERING<br>\n",
    "https://openreview.net/pdf?id=HyNxRZ9xg CAT2VEC: LEARNING DISTRIBUTED REPRESENTATION OF MULTI-FIELD CATEGORICAL DATA<br>\n",
    "https://arxiv.org/pdf/1604.06737v1.pdf Entity Embeddings of Categorical Variables<br>\n",
    "https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture Embeddings<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# target encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Второе.\n",
    "К тебе в задачу обязательно закрадутся сучьи категориальные признаки, вот самые популярные методы их поиметь, так же как жизнь поимела тебя\n",
    "https://www.datacamp.com/community/tutorials/categorical-data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
